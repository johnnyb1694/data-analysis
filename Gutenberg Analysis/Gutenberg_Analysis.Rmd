---
title: 'Project Gutenberg: Example Analysis'
author: "Johnny Breen"
date: "09/04/2019"
output: html_document
---
# Preamble on 'Project Gutenberg'

It's been a little while since I've practiced text mining so I thought I would analyse some textual data provided by Project Gutenberg. In case you were not aware, Project Gutenberg consists of various articles and literary contributions (amongst other things) written, in the main, by authors whose works are now in the US public domain and largely exempt from any copyright infringement. For the UK, the law is that copyright applies to somebody else's work until [70 years after their date of death](https://www.gov.uk/copyright/how-long-copyright-lasts). 

Luckily for us, there is a package in R, [gutenbergr](https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html), which allows you to easily download and access written articles stored within the Project Gutenberg database within the click of a button!

# Background

Upon inspecting the various repositories of data available to me in Project Gutenberg, I became rather fascinated by the prospect of analysing Abraham Lincoln's various letters and writings composed between the years 1832 and 1865 (Lincoln's year of death). I don't particularly know much about Abraham Lincoln, I'm ashamed to say, but I wanted to see how much I could learn about the former president by simply performing a short text-mining exercise on the extensive repository of written letters he managed to create over the course of his political career.

# Preliminaries

First, we load the necessary packages for this short analysis - I've added a comment besides each package to explain how it is used within the remainder of this article:

```{r message=FALSE, warning=FALSE}
library(gutenbergr) # allows you to download articles stored within the Project Gutenberg database
library(tidyverse) # includes various data manipulation packages, most notably 'tidyr' and 'dplyr'
library(tidytext) # facilitates text mining
library(broom) # allows you to 'tidy' model outputs into a tibble format
library(igraph) # allows you to plot 'network' objects
library(ggraph) # can be thought of as an 'igraph' addon which simply translates base plots into ggplot formats

theme_set(theme_minimal()) # sets a global theme for ggplot2 visualisations
```

# Data cleansing & exploratory data analysis



```{r}
lincoln_raw <- gutenberg_download(2653:2659) # ids 2653 to 2659 encode Lincoln's writings between 1832 and 1865
lincoln_mappings <- gutenberg_metadata %>%
  select(gutenberg_id, title) %>%
  filter(between(gutenberg_id, 2653, 2659))

lincoln_mappings
```

First let's clean the data.

```{r message=FALSE, warning=FALSE}
lincoln_clean <- lincoln_raw %>%
  unnest_tokens(output = word, input = text) %>%
  mutate(unique_id = row_number(), date = ifelse(str_detect(word, regex("\\d{4}")), word, NA_character_)) %>%
  fill(date) %>%
  mutate_at(vars(date), .funs = funs(as.numeric)) %>%
  filter(between(date, 1832, 1865), 
         !str_detect(word, "lincoln"),
         str_detect(word, "[a-z]")) %>%  
  anti_join(stop_words)

lincoln_clean
```

Let's review the most commonly used words:

```{r}
lincoln_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20, n) %>%
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, colour = n)) +
  geom_point() +
  geom_segment(aes(x = word, xend = word, y = 0, yend = n)) +
  scale_colour_gradient(low = "slateblue2", high = "tomato2") +
  coord_flip() +
  labs(x = "Word",
       y = "Frequency",
       colour = "Count",
       title = "Word frequency analysis of Lincoln's writings",
       subtitle = "Key themes include slavery amongst others")
```

Let's track his sentiments over time:

```{r message=FALSE}
lincoln_clean %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date) %>%
  summarise(avg_sentiment = mean(score, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = avg_sentiment)) +
  geom_point(aes(colour = (between(avg_sentiment, -0.5, 0.5)), size = abs(avg_sentiment)), show.legend = FALSE) +
  geom_line(alpha = 0.75, colour = "black") +
  scale_x_continuous(breaks = seq(1830, 1865, 5)) +
  expand_limits(y = c(-1, 1)) +
  labs(x = "Year of study",
       y = "Mental score",
       title = "Progression of Lincoln's mental state over time",
       subtitle = "Notice the downturn pre-1855 followed by a sharp upturn towards 1865")
```

It might also be interesting to track whether there are any words used by Lincoln which became more prominent or less prominent over time.

To achieve this we fit a logistic regression model to each 'word'. For example, consider the word government:

```{r}
date_counts <- lincoln_clean %>%
  count(date)

lincoln_clean %>%
  add_count(date, word) %>%
  inner_join(date_counts, by = c("date" = "date")) %>%
  filter(word == "government") %>%
  arrange(date) %>%
  group_by(date, word) %>%
  summarise(successes = sum(n.x), trials = sum(n.y))

lincoln_reg_used_words <- lincoln_clean %>%
  count(word, date) %>%
  group_by(word) %>%
  summarise(total_appearances = n()) %>%
  filter(total_appearances > 20) %>% 
  pull(word) # words with at least ten years of data
  
```

We consider the column `n` to reflect the number of mentions (succcesses) in a given year out of the total words written in that year, `nn` (number of trials).

```{r}
lincoln_counts <- lincoln_clean %>%
  filter(word %in% lincoln_reg_used_words) %>%
  select(date, word) %>%
  add_count(date, word) %>%
  inner_join(date_counts, by = c("date" = "date")) %>%
  group_by(date, word) %>%
  summarise(successes = sum(n.x), trials = sum(n.y)) %>%
  ungroup() %>%
  mutate(prop_success = successes / trials) %>%
  filter(trials > 1000) # for statistical significance

lincoln_slopes <- lincoln_counts %>%
  nest(-word) %>%
  mutate(log_mod = map(data, ~ glm(cbind(successes, trials - successes) ~ date, family = "binomial", data = .))) %>%
  unnest(map(log_mod, broom::tidy)) %>%
  filter(term == "date") %>%
  arrange(desc(estimate))

lincoln_top_growing <- lincoln_slopes %>%
  top_n(10, estimate) %>%
  pull(word)

lincoln_top_shrinking <- lincoln_slopes %>%
  top_n(-10, estimate) %>%
  pull(word)
```

Let's plot these trends on a separate graph:

```{r}
# words which Lincoln began to use more towards the end of his writings
lincoln_counts %>%
  filter(word %in% lincoln_top_growing) %>%
  ggplot(aes(x = date, y = prop_success, colour = word)) +
  geom_line(show.legend = FALSE) +
  facet_wrap(~word) +
  labs(x = "Year",
       y = "Frequency of usage",
       title = "Top 10 highest growing words",
       subtitle = "War and Washington, amongst others, make noticeable appearances")

# words which Lincoln started to use less frequently towards the end of his writings
lincoln_counts %>%
  filter(word %in% lincoln_top_shrinking) %>%
  ggplot(aes(x = date, y = prop_success, colour = word)) +
  geom_line(show.legend = FALSE) +
  facet_wrap(~word) +
  labs(x = "Year",
     y = "Frequency of usage",
     title = "Top 10 highest shrinking words",
     subtitle = "Themes of liberty, such as 'free' and 'slavery', appear to decrease in usage over time")
```

```{r}
lincoln_clean_bigrams <- lincoln_raw %>% 
  unnest_tokens(output = word, input = text, token = "ngrams", n = 2) %>%
  mutate(unique_id = row_number(), date = ifelse(str_detect(word, regex("\\d{4}")), 
                                                 str_extract(word, regex("\\d{4}")), 
                                                 NA_character_)) %>%
  fill(date) %>%
  mutate_at(vars(date), .funs = funs(as.integer)) %>%
  filter(between(date, 1832, 1865), 
         !str_detect(word, "lincoln"),
         str_detect(word, "[a-z]")) # filter anything that could not be interpreted as a reasonable 'date' 

lincoln_clean_bigrams %>%
  separate(col = word, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE) %>%
  filter(n > 50) %>%
  graph_from_data_frame(directed = TRUE) %>%
  ggraph(layout = "nicely") +
  geom_node_point(colour = "lightblue", size = 4) +
  geom_edge_link(aes(edge_alpha = n), 
                 arrow = arrow(type = "closed", length = unit(.15, "inches")), 
                 end_cap = circle(.07, 'inches'), show.legend = FALSE) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  labs(title = "Lincoln's most common bigrams",
       subtitle = "The transparency of the arrow reflects the frequency of usage")
  
```


